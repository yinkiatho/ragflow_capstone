2025-03-28 11:20:44,275 - INFO - _client - _client.py:1025 - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-03-28 11:20:50,625 - INFO - testing_goldens - testing_goldens.py:119 - Loaded total of 791 Chunks
2025-03-28 11:20:52,211 - INFO - testing_goldens - testing_goldens.py:219 - Testing Attack @ 2025-03-28 11:20:52.211413+08:00
2025-03-28 11:21:23,016 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:21:35,656 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:21:50,690 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:22:05,289 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:22:18,177 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:22:33,788 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:22:33,825 - ERROR - RagFlow - RagFlow.py:288 - Error in model call back guardrails: The callable `fn` passed to `Guard(fn, ...)` failed with the following error: `1 validation error for LLMResponse
stream_output
  Input should be an instance of Iterator [type=is_instance_of, input_value=<async_generator object R...k at 0x0000024C63F3B640>, input_type=async_generator]
    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of`. Make sure that `fn` can be called as a function
that accepts a prompt string, **kwargs, and returns a string.
 If you're using a custom LLM callable, please see docs
 here: https://go.guardrailsai.com/B1igEy3
