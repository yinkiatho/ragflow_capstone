2025-03-28 11:30:00,115 - INFO - _client - _client.py:1025 - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-03-28 11:30:06,758 - INFO - testing_goldens - testing_goldens.py:119 - Loaded total of 791 Chunks
2025-03-28 11:30:08,296 - INFO - testing_goldens - testing_goldens.py:219 - Testing Attack @ 2025-03-28 11:30:08.296463+08:00
2025-03-28 11:30:31,200 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:30:44,593 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:30:59,800 - INFO - _client - _client.py:1025 - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-03-28 11:33:04,021 - ERROR - RagFlow - RagFlow.py:320 - Error in model call back guardrails: The callable `fn` passed to `Guard(fn, ...)` failed with the following error: `1 validation error for LLMResponse
stream_output
  Input should be an instance of Iterator [type=is_instance_of, input_value="The answer you are looki...of all individuals.  \n", input_type=str]
    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of`. Make sure that `fn` can be called as a function
that accepts a prompt string, **kwargs, and returns a string.
 If you're using a custom LLM callable, please see docs
 here: https://go.guardrailsai.com/B1igEy3
